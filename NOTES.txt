TODO 

	- [ ] add some minimal resource requests

	- [ ] prepare an issue documenting the work performed, what to do next,
	      the approach, etc

	- [ ] add label that identifies the set of pods as owned by the "worker"

		e.g., `org.concourse-ci.worker=blabla`

		this way, each "registration" takes care of itself
			(and we can deal with getting rid of leftovers from "stalled"  --> "retired")

	- [ ] worker lifecycle

		- [ ] retire?
			
			this makes sense... e.g., when it comes to getting a cluster out
				-> clean the environment w/ a certain grace period
			
		- [ ] land?
			e.g., when peforming a "known" maintenance?

				==> would mark the target to not receive any
				"creation calls", I guess?

		- [ ] stalled?

			?

	- [x] get back the exit status of the script executions

	- [x] consume "rootfs" images (concourse own format)

	- [x] configure client-go's BPS

	- [x] perform proper `check` in `image_resource` image fetching

	- [x] container state transition

	- [x] ignore task caches in list of artifact inputs

	- [x] put step

	- [x] task outputs

	- [x] garbage collection of containers

		- currently, `report`, list destroying, destroy.
				|
				'--> we probably don't need to call an API endpoint

					==> refactored that part of the code into a "handles syncer"

						- can either be used in the API (as part of responding 
						  to requests), or other components


		sweeper:

			1. lists the containers that it has	 		(ok)
			2. reports those					(not yet)
			3. gathers the list of containers to destroy		(not yet)
			4. destroys each.					(ok)



	- [ ] see if we can influence scheduling by putting annotations / labels
	      that identify the builds themselves

	- [ ] test custom resource types

	- [ ] pod spec stuff

		- [ ] container limits
		- [ ] user in task definition
		- [ ] privileged
			- demonstrate how we're able to go from source to container
			  image without the need of any privileges (that's a very basic
			  thing we should be able to do nowadays)


	- [ ] add logs to where it's important to extract info from

	- [ ] multiple namespaces / clusters?

		- argocd: https://github.com/argoproj/argo-cd/issues/1673

		- spinnaker!  https://docs.armory.io/spinnaker-install-admin-guides/add-kubernetes-account/

			- each "worker" registers against the cluster by providing its own kubeconfig,
			  that has everything set to reach out to the cluster/target


				=> you create the svc account, token, etc, and then give us
				   the locations of the kubeconfigs.

					==> being a file on disk, that could come from a set of
					    secrets.


			use SelfSubjectAccessReview to determine if we can do what we want
			https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access


		- [ ] tags?
			
			==> we could leverage the notion of `teams` that we have
			ALREADY and leverage that in the form of a "pool" that
			selects based on that and tags


	- [ ] should the worker regitration bring `nodeSelector` stuff?

	- [ ] caches?

	- [ ] fly hijack

	- [ ] fly execute

	- [ ] having non-k8s workers together
		- as a user, would you even choose that?
